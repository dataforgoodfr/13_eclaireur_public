# Pipeline d'int√©gration des donn√©es

L'int√©gralit√© du contenu du dossier `./back/` concerne la partie backend du projet.



## Table des mati√®res

- [Pipeline d'int√©gration des donn√©es](#pipeline-dint√©gration-des-donn√©es)
  - [Table des mati√®res](#table-des-mati√®res)
  - [Structure du back](#structure-du-back)
  - [Flux de Donn√©es](#flux-de-donn√©es)
  - [Contribuer](#contribuer)
    - [Acces Repo](#acces-repo)
    - [Environnement de d√©veloppement](#environnement-de-d√©veloppement)
      - [Installation de Poetry avec pipx](#installation-de-poetry-avec-pipx)
      - [Installation de Poetry avec le depot officiel](#installation-de-poetry-avec-le-depot-officiel)
      - [Utiliser Poetry](#utiliser-poetry)
      - [Utiliser un venv python](#utiliser-un-venv-python)
  - [Lancer les precommit hook localement](#lancer-les-precommit-hook-localement)
  - [Utiliser Tox pour tester votre code](#utiliser-tox-pour-tester-votre-code)
  - [Executer PostgreSQL localement avec docker](#executer-postgresql-localement-avec-docker)
    - [Installer docker](#installer-docker)
    - [D√©marrer une instance](#d√©marrer-une-instance)
  - [Lancer le script](#lancer-le-script)
    - [Sur des donn√©es de test](#sur-des-donn√©es-de-test)
    - [Sur l'ensemble des donn√©es](#sur-lensemble-des-donn√©es)
  - [Licenses](#licenses)
    - [Code](#code)
    - [Donn√©es et Analyses](#donn√©es-et-analyses)





## Structure du back

- `data/`: dossier pour stocker les donn√©es du projet, organis√©es en sous-dossiers

    - `communities/`: informations sur les collectivit√©s
    - `datasets/`: donn√©es r√©cup√©r√©es et filtr√©es
    - `processed_data/`: donn√©es trait√©es et pr√™tes pour l'analyse
- `scripts/`: dossier pour les scripts Python du projet, organis√©s en sous-dossiers
    - `workflow/` : script g√©rant le workflow g√©n√©ral
    - `communities/`: scripts pour la gestion des collectivit√©s
    - `datasets/`: scripts pour le scrapping et le filtrage des donn√©es
    - `data_processing/`: scripts pour le traitement des donn√©es
    - `analysis/`: scripts pour l'analyse des donn√©es (vide √† date)
    - `loaders/`: scripts de t√©l√©chargement de fichiers
    - `utils/`: scripts utilitaires et helpers
- `main.py`: script principal pour ex√©cuter les scripts du projet
- `config.yaml`: fichier de configuration pour faire tourner `main.py`.
 - `.gitignore`: fichier contenant les r√©f√©rences ignor√©es par git
- `README.md`: ce fichier

---

# üóÇÔ∏è Pipeline de Traitement des Donn√©es

##  Objectif

Le pipeline a pour but de :

- Collecter, nettoyer, uniformiser et enrichir des jeux de donn√©es publics, notamment sur les **subventions territoriales**.
- Rendre ces donn√©es exploitables via une **interface web** destin√©e aux citoyens, journalistes, chercheurs ou associations et les march√©s publics.

---

##  Architecture G√©n√©rale

Le pipeline de traitement des donn√©es s‚Äôarticule en **trois √©tapes**, allant de la r√©cup√©ration brute des donn√©es jusqu‚Äô√† leur standardisation finale dans un format exploitable: 

1. **Constitution des bases principales**
2. **Enrichissements via des plateformes OpenData**
3. **Structuration finale et validation**



üîó **Sch√©ma complet du pipeline** (POC Anticor) :  
 [Lien Excalidraw](https://excalidraw.com/#room=f3a228d37457f02aa822,PmpHg9tvB0P0Zs4KuLExIA)

---

## Sources de donn√©es utilis√©es

Les donn√©es sont agr√©g√©es √† partir de sources publiques, fiables et actualis√©es, notamment :

| Source            | Description                                                                 | Exemple d‚Äôusage                        |
|------------------|------------------------------------------------------------------------------|----------------------------------------|
| **INSEE (Sirene)**        | Donn√©es l√©gales sur les entreprises, structures publiques et collectivit√©s (codes SIREN/SIRET, formes juridiques, NAF, etc.) | Identification et typage des entit√©s   |
| **ODF (Observatoire des Finances)** | Donn√©es financi√®res consolid√©es des collectivit√©s locales         | Budgets, typologies budg√©taires         |
| **DataGouv API**         | M√©tacatalogue et ressources ouvertes, sans h√©bergement direct        | Recherche de fichiers Opendata annexes |
| **Data INSEE (Codes g√©ographiques)** | Codes g√©ographiques, d√©mographie, codes r√©gion/d√©partement      | Appariement g√©ographique                |

> **Note** : Bien que `data.gouv.fr` soit la plateforme de centralisation, les donn√©es y sont g√©n√©ralement r√©f√©renc√©es mais pas h√©berg√©es. Les appels se font donc majoritairement directement aupr√®s de l‚ÄôINSEE ou des sources finales (OFGL, DGFIP, etc.).

---

## Traitements appliqu√©s

Le pipeline `communities` applique les √©tapes suivantes :

1. **Chargement des donn√©es INSEE (Sirene)**
   - Donn√©es SIREN/SIRET + formes juridiques + NAF
   - Normalisation des entit√©s juridiques
   - Nettoyage des doublons

2. **R√©cup√©ration des donn√©es ODF**
   - Donn√©es financi√®res locales, typologie des collectivit√©s
   - Mappage avec les identifiants INSEE/SIREN
   - Calcul de m√©triques de r√©f√©rence : population, d√©penses, etc.

3. **Enrichissement via DataGouv API**
   - Appel de l‚ÄôAPI pour extraire des ressources annexes (m√©triques, subventions, etc.)
   - Appariement via des correspondances (code commune, code postal, etc.)

4. **Fusion, consolidation et cr√©ation du fichier `communities.parquet`**
   - Regroupement des donn√©es par SIREN
   - Ajout des m√©tadonn√©es utiles (cat√©gorie, statut, r√©gion, EPCI, etc.)
   - Validation finale de structure

---

##  Sp√©cificit√©s techniques et bonnes pratiques

- Le pipeline repose sur une architecture modulaire orchestr√©e par un gestionnaire de workflows (`workflow_manager.py`)
- Tous les fichiers sources sont convertis au format **Parquet** pour une meilleure performance en lecture/√©criture.
- La logique de fusion des sources repose principalement sur les **codes SIREN/SIRET** et les **codes g√©ographiques INSEE**.

---

## Flux de Donn√©es

Le diagramme suivant illustre le flux de traitement des donn√©es orchestr√© par le script `workflow_manager.py`.

### √âtape 1: Ex√©cution des Workflows de Base

#### Workflows Ind√©pendants

1.  **`CPVLabelsWorkflow`**:
    *   **R√¥le**: Charge les libell√©s CPV (Common Procurement Vocabulary).
    *   **Entr√©es**: Un fichier distant sp√©cifi√© par `cpv_labels.url` dans la configuration.
    *   **Sortie**: `data/cpv_labels.parquet`

2.  **`SireneWorkflow`**:
    *   **R√¥le**: Traite les donn√©es SIRENE pour les informations l√©gales sur les entit√©s fran√ßaises.
    *   **Entr√©es**:
        *   Un fichier zip distant depuis `sirene.url`.
        *   Plusieurs fichiers Excel distants pour les codes NAF depuis `sirene.xls_urls_naf`.
        *   Un fichier Excel distant pour les cat√©gories juridiques depuis `sirene.xls_urls_cat_ju`.
    *   **Sortie**: `data/sirene.parquet`

3.  **`FinancialAccounts`**:
    *   **R√¥le**: Agr√®ge les comptes financiers des collectivit√©s.
    *   **Entr√©es**:
        *   Un fichier CSV local (`financial_accounts.files_csv`) qui liste les fichiers de donn√©es √† t√©l√©charger et √† traiter.
        *   Un fichier CSV local (`financial_accounts.columns_mapping`) pour le mappage des colonnes.
    *   **Sortie**: `data/financial_accounts.parquet`

4.  **`ElectedOfficialsWorkflow`**:
    *   **R√¥le**: Collecte des informations sur les √©lus.
    *   **Entr√©es**: R√©cup√®re la liste des ressources depuis l'API DataGouv pour le jeu de donn√©es `5c34c4d1634f4173183a64f1`.
    *   **Sortie**: `data/elected_officials.parquet`

5.  **`DeclaInteretWorkflow`**:
    *   **R√¥le**: Traite les d√©clarations d'int√©r√™ts des √©lus.
    *   **Entr√©es**: Un fichier XML distant depuis `declarations_interet.url`.
    *   **Sortie**: `data/declarations_interet.parquet`

6.  **`OfglLoader`**:
    *   **R√¥le**: Charge les donn√©es de l'OFGL (Observatoire des finances et de la gestion publique locales).
    *   **Entr√©es**: Un fichier CSV local (`ofgl.urls_csv`) contenant les URLs √† t√©l√©charger.
    *   **Sortie**: `data/ofgl.parquet`

#### Workflows D√©pendants

7.  **`CommunitiesSelector`**:
    *   **R√¥le**: Cr√©e une liste organis√©e de collectivit√©s fran√ßaises.
    *   **Entr√©es**:
        *   `data/ofgl.parquet`
        *   `data/sirene.parquet`
        *   Un fichier distant pour les donn√©es ODF depuis `communities.odf_url`.
        *   Un fichier distant pour les donn√©es EPCI depuis `communities.epci_url`.
        *   R√©cup√®re les m√©triques g√©ographiques depuis l'API DataGouv pour le jeu de donn√©es sp√©cifi√© dans `communities.geo_metrics_dataset_id`.
    *   **Sortie**: `data/communities.parquet`

8.  **`DataGouvCatalog`**:
    *   **R√¥le**: R√©cup√®re et traite l'int√©gralit√© du catalogue DataGouv.
    *   **Entr√©es**:
        *   `data/communities.parquet`
        *   R√©cup√®re le catalogue depuis l'API DataGouv (jeu de donn√©es `5d13a8b6634f41070a43dff3`) ou une URL directe depuis `datagouv_catalog.catalog_url`.
    *   **Sortie**: `data/datagouv_catalog.parquet`

9.  **`MarchesPublicsWorkflow`**:
    *   **R√¥le**: Agr√®ge les donn√©es des march√©s publics.
    *   **Entr√©es**:
        *   `data/datagouv_catalog.parquet` (pour trouver les ressources du jeu de donn√©es `5cd57bf68b4c4179299eb0e9`).
        *   Un sch√©ma JSON distant depuis `marches_publics.schema`.
    *   **Sortie**: `data/marches_publics.parquet`

10. **`DataGouvSearcher`**:
    *   **R√¥le**: Recherche dans le catalogue DataGouv les jeux de donn√©es relatifs aux subventions.
    *   **Entr√©es**: `data/datagouv_catalog.parquet`.
    *   **Sortie**: `data/datagouv_search.parquet`

11. **`CommunitiesContact`**:
    *   **R√¥le**: R√©cup√®re les informations de contact des administrations fran√ßaises.
    *   **Entr√©es**:
        *   `data/datagouv_catalog.parquet` (pour trouver la ressource du jeu de donn√©es `53699fe4a3a729239d206227`).
        *   Une URL directe depuis `communities_contacts.url` peut √©galement √™tre utilis√©e.
    *   **Sortie**: `data/communities_contacts.parquet`

```mermaid
graph TD

    subgraph "√âtape 1: Ex√©cution des Workflows de Base"
        direction LR

        A[CPVLabelsWorkflow] -- reads from config --> A_OUT((data/cpv_labels.parquet))
        B[SireneWorkflow] -- reads from config --> B_OUT((data/sirene.parquet))
        C[FinancialAccounts] -- reads from config --> C_OUT((data/financial_accounts.parquet))
        D[ElectedOfficialsWorkflow] -- reads from DataGouv API --> D_OUT((data/elected_officials.parquet))
        DI[DeclaInteretWorkflow] -- reads from config --> DI_OUT((data/declarations_interet.parquet))
        E[OfglLoader] -- reads from config --> E_OUT((data/ofgl.parquet))

        F[CommunitiesSelector]
        G[DataGouvCatalog]
        H[MarchesPublicsWorkflow]
        I[DataGouvSearcher]
        J[CommunitiesContact]

        E_OUT --> F
        B_OUT --> F
        F --> F_OUT((data/communities.parquet))

        F_OUT --> G
        G -- reads from DataGouv API --> G
        G --> G_OUT((data/datagouv_catalog.parquet))

        G_OUT --> H
        H --> H_OUT((data/marches_publics.parquet))

        G_OUT --> I
        I --> I_OUT((data/datagouv_search.parquet))

        G_OUT --> J
        J --> J_OUT((data/communities_contacts.parquet))
    end

    subgraph "√âtape 2: Traitement des Subventions"
        K[process_subvention]
        L[SingleUrlsBuilder]
        M[TopicAggregator]
        N((Donn√©es de subvention<br>agr√©g√©es en sortie))
    end

    I_OUT -- Fichier Parquet --> K
    L -- URLs --> K
    K -- Donn√©es combin√©es --> M
    M --> N

    J_OUT -.-> K
```

## Contribuer

- Rappel: La contribution du projet se fait par l'interm√©diaire de Data 4 Good. Il est n√©cessaire de se rapprocher du Slack d√©di√©, canal 13_eclair_public, pour toutes questions.
- Pour les nouveaux arrivants: Pensez √† vous pr√©senter dans les canaux d√©di√©s, participez aux points hebdo qui on lieu le jeudi.


### Acces Repo


``` bash
# Copier le repo en local
git clone https://github.com/dataforgoodfr/13_eclaireur_public.git
```


### Environnement de d√©veloppement


> Le projet n√©cessite l'installation de Python 3.13 et de Poetry au minimum en version 2.


Plusieurs [m√©thodes d'installation](https://python-poetry.org/docs/#installation) sont d√©crites dans la documentation de poetry dont:

- avec pipx
- avec l'installateur officiel

Chaque m√©thode a ses avantages et inconv√©nients. Par exemple, la m√©thode pipx n√©cessite d'installer pipx au pr√©able, l'installateur officiel utilise curl pour t√©l√©charger un script qui doit ensuite √™tre ex√©cut√© et comporte des instructions sp√©cifiques pour la completion des commandes poetry selon le shell utilis√© (bash, zsh, etc...).

L'avantage de pipx est que l'installation de pipx est document√©e pour linux, windows et macos. D'autre part, les outils install√©es avec pipx b√©n√©ficient d'un environment d'ex√©cution isol√©, ce qui est permet de fiabiliser leur fonctionnement. Finalement, l'installation de poetry, voire d'autres outils est relativement simple avec pipx.

Cependant, libre √† toi d'utiliser la m√©thode qui te convient le mieux ! Quelque soit la m√©thode choisie, il est important de ne pas installer poetry dans l'environnement virtuel qui sera cr√©√© un peu plus tard dans ce README pour les d√©pendances de la base de code de ce repo git.

#### Installation de Poetry avec pipx

Suivre les instructions pour [installer pipx](https://pipx.pypa.io/stable/#install-pipx) selon ta plateforme (linux, windows, etc...)

Par exemple pour Ubuntu 23.04+:

    sudo apt update
    sudo apt install pipx
    pipx ensurepath

Pour macos:

    brew install pipx
    pipx ensurepath

[Installer Poetry avec pipx](https://python-poetry.org/docs/#installing-with-pipx):

    pipx install poetry



#### Installation de Poetry avec le depot officiel

L'installation avec l'installateur officiel n√©cessitant quelques √©tapes suppl√©mentaires,
se r√©f√©rer √† la [documentation officielle](https://python-poetry.org/docs/#installing-with-the-official-installer).


#### Utiliser Poetry

``` bash
# Installer les d√©pendances
poetry install
# Mettre √† jour les d√©pendances
poetry update
```


#### Utiliser un venv python

<span style="color: darkred;">Si vous pr√©f√©rez utiliser un venv python, suivez les instructions suivantes:</span>

``` bash
python3 -m venv .venv
source .venv/bin/activate
# Il vous sera necessaire de vous assurer d'installer les d√©pendances requises, poetry ne g√©n√©rant pas de requirements.txt par d√©faut.
# Actuellement, aucun support n'est propos√© pour les venv python.
```



## Lancer les precommit hook localement

[Installer les precommit](https://pre-commit.com/)
``` bash
pre-commit run --all-files
```


## Utiliser Tox pour tester votre code
``` bash
tox -vv
```

## Executer PostgreSQL localement avec docker
Par d√©faut, le script sauvegarde ses r√©sultats dans une base PostgreSQL locale. Il est donc n√©c√©saire d'√©x√©cuter localement une instance, ce qu'il est possible de faire avec docker.

> Vous pouvez d√©sactiver cette fonctionnalit√© en changeant `workflow.save_to_db: False` dans la config.

### Installer docker
Se reporter √† la [documentation](https://docs.docker.com/engine/install/) docker.

### D√©marrer une instance
Depuis un terminal:

    docker compose -f docker-compose.yaml up -d

## Lancer le script
### Sur des donn√©es de test

    poetry run python back/main.py -f back/config-test.yaml

### Sur l'ensemble des donn√©es

    poetry run python back/main.py

## Lancer le script dans un conteneur
Pr√©-requis :
- Docker 
- Task 
#### Installer task
https://taskfile.dev/installation/

#### Construire l'image 

    task docker:build

#### Lancer le conteneur

    task docker:run

## Licenses

### Code

The code in this repository is licensed under the [MIT License](./../LICENSE)

### Donn√©es et Analyses

Sauf indication contraire, les donn√©es et analyses de ce d√©p√¥t sont sous licence [Creative Commons Attribution 4.0 International (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/).
