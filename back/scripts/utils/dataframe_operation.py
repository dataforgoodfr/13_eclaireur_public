import logging
import pandas as pd

"""
This script contains functions to manipulate DataFrames.
1 - Merging duplicate columns
2 - Renaming columns
3 - Casting data based on a schema
4 - Detecting the first row index where the data starts
5 - Detecting the first column index where the data starts
"""


# Function to merge duplicate columns in a DataFrame
def merge_duplicate_columns(df):
    df.columns = df.columns.astype(str)
    duplicate_columns = df.columns[df.columns.duplicated(keep=False)]
    for col in duplicate_columns.unique():
        cols_to_merge = df.filter(like=col, axis=1)
        df[col] = cols_to_merge.apply(lambda x: " / ".join(x.dropna().astype(str)), axis=1)
        df.drop(cols_to_merge.columns[1:], axis=1, inplace=True)
    return df


# Function to rename columns in a DataFrame
def safe_rename(df, schema_dict):
    schema_dict_copy = schema_dict.copy()
    for original_name, official_name in schema_dict.items():
        if official_name in df.columns and original_name != official_name:
            del schema_dict_copy[original_name]
    df.rename(columns=schema_dict_copy, inplace=True)


# Function to cast the data in a DataFrame based on a schema (a DataFrame with two columns: 'name' and 'type')
def cast_data(data, schema, name_tag, clean_column_name_for_comparison=None):
    logger = logging.getLogger(__name__)
    # Dict between schema types and pandas types
    # https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#basics-dtypes
    type_dict = {
        "string": "string",
        "integer": "Int64",
        "number": "float64",
        "boolean": "boolean",
        "date": "datetime64[ns]",
    }

    # Create a dictionary of original column names mapped to their cleaned versions for comparison
    if clean_column_name_for_comparison:
        original_to_cleaned_names = {
            col: clean_column_name_for_comparison(col) for col in data.columns
        }
    else:
        # If no cleaning function is provided, use the exact same column names
        original_to_cleaned_names = {col: col for col in data.columns}

    # Create a new dataframe with the same shape and columns as data
    casted_data = pd.DataFrame(columns=data.columns)

    # Go through each column in the data to cast it
    for original_name, cleaned_name in original_to_cleaned_names.items():
        # if column name is not in schema['name'].values, keep the exact same column
        if cleaned_name not in schema[name_tag].values:
            casted_data[original_name] = data[original_name]
        # if column name is in schema['name'].values, cast the column with the paired schema['type'] value
        else:
            # get the schema type for the column
            schema_type = schema.loc[schema[name_tag] == cleaned_name, "type"].values[0]
            # translate the schema type to pandas type using type_dict
            pandas_type = type_dict[schema_type]
            # clean & cast the column to the pandas type, based on internal function
            casted_data[original_name] = _clean_and_cast_col(data[original_name], pandas_type)
            logger.info(f"Column '{original_name}' has been casted to '{pandas_type}'")
    return casted_data


# Internal function to clean and cast a column based on its schema type
def _clean_and_cast_col(col, pandas_type):
    logger = logging.getLogger(__name__)
    # Make a copy of the orginal column
    col_original = col.copy()

    if pandas_type == "float64":
        # Replace ',' by '.' to handle French numeric format and remove spaces (including non-breaking spaces)
        col = col.replace({",": ".", "\\s+": "", "%": ""}, regex=True)
        col = pd.to_numeric(col, errors="coerce")  # Coerce errors will be set to NaN
    elif pandas_type == "datetime64[ns]":
        # Convert to datetime, with utc=true, errors will be coerced to NaT
        col = col.astype(str)
        col = col.apply(_parse_date)
        # Check if the data is timezone-aware
        if col.dt.tz is not None:
            col = col.dt.tz_localize(None)
    elif pandas_type == "string":
        col = col.astype(str)
        col = col.str.strip()
        col = col.astype(str)
    elif pandas_type == "Int64":
        # Arrondir les valeurs flottantes
        col = col.apply(lambda x: round(x) if not pd.isna(x) and isinstance(x, float) else x)
        # Convert to integer, note that 'Int64' can handle NaN values
        col = pd.to_numeric(col, errors="coerce").round().astype("Int64")
    elif pandas_type == "boolean":
        col = col.str.replace(r"\s+", "", regex=True).str.lower()
        # Convert to boolean, True for 'oui', False for 'non', case insensitive
        col = col.str.lower().map({"oui": True, "non": False, "false": False, "true": True})

    # Compare the original column with the copy to identify coerced values
    coerced_indices = col_original.index[(col_original.notnull()) & (col_original != col)]
    coerced_values = col_original.loc[coerced_indices]

    if not coerced_values.empty:
        # Log the coerced values and relevant information
        for index, value in coerced_values.items():
            if ("nan" not in str(value)) & (pd.isna(col.loc[index])):
                logger.error(
                    f"Value '{value}' supposed to be a '{pandas_type}' was coerced to {col[index]}"
                )

    return col.astype(pandas_type)  # Convert to specified pandas type


# Internal function to parse a date string
def _parse_date(date_str):
    try:
        # dateutil parser can handle different formats
        return pd.to_datetime(date_str, utc=True)
    except ValueError:
        # Handle the error if the date format is not recognized
        return pd.NaT  # Return 'Not a Time' for unparseable formats


# Function to detect the first row index in a DataFrame where the data starts
def detect_skiprows(df):
    # Find the last non-empty column
    last_non_empty_col = len(df.dropna(how="all", axis=1).columns) - 1
    # Get the first row index in this column
    first_row = df.iloc[:, last_non_empty_col].first_valid_index()
    return first_row


# Function to detect the first column index in a DataFrame where the data starts
def detect_skipcolumns(df):
    df_transposed = df.transpose().reset_index(drop=True)
    return detect_skiprows(df_transposed)
